import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import pickle

# Load GPT-2 model + tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Fix padding issue
tokenizer.pad_token = tokenizer.eos_token

# Sample training data
train_texts = [
    "This research explores the applications of deep learning in medical imaging...",
    "Artificial Intelligence has shown great potential in natural language processing tasks...",
    "This paper presents a predictive model for climate change impact assessment...",
]

# Encode training data with padding
inputs = tokenizer(train_texts, return_tensors="pt", padding=True, truncation=True)

# Simple fine-tuning step (for demo)
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
loss.backward()  # tiny update

# Save model + tokenizer as .pkl
with open("ai_abstract_generator.pkl", "wb") as f:
    pickle.dump({"model": model, "tokenizer": tokenizer}, f)

print("âœ… Model saved as ai_abstract_generator.pkl")
